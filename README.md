# Backpropagation and Training of Neural Networks:
Implemented micrograd following Andrej Karpathy's tutorial, focusing on understanding backpropagation and training neural networks from scratch.

# Bigram Character-Level Language Model:
Developed a bigram language model using PyTorch, focusing on model training, sampling, and evaluation using negative log likelihood.

# Multilayer Perceptron (MLP) Character-Level Language Model:
Implemented an MLP-based language model, exploring machine learning basics like model training, hyperparameter tuning, and overfitting mitigation.

# Internals of MLPs and Batch Normalization:
Explored MLP internals, focusing on activation statistics, gradient flows, and introduced Batch Normalization to stabilize training.

# Manual Backpropagation Through MLP Layers:
Implemented backpropagation manually through a 2-layer MLP, gaining deep insight into how gradients flow through neural network layers.

# Hierarchical MLP Architecture (WaveNet-like):
Extended a 2-layer MLP to a hierarchical tree-like structure similar to WaveNet, exploring deeper neural network architectures and torch.nn functionality.

# Generatively Pretrained Transformer (GPT) Implementation:
Built a GPT model following "Attention is All You Need", exploring autoregressive language modeling and connections to modern AI models like ChatGPT.
